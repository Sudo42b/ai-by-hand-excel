18\. Sora's Diffusion Transformer (DiT)
=======================================

### Can you calculate this by hand? âœï¸

[![Tom Yeh's avatar](https://substackcdn.com/image/fetch/w_36,h_36,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc1c7fa3c-71fa-44a4-bf32-d68e25f2e534_800x800.jpeg)

](https://substack.com/@tomyeh)

[Tom Yeh](https://substack.com/@tomyeh)

Feb 20, 2024

28

[Share
](javascript:void(0))

*AI by Hand âœï¸ 2025 Calendar ğŸ—“ï¸ is Now Available! Visit ourÂ [online store](http://by-hand.ai/store)!*

OpenAI's Sora is finally released! It took over the Internet when it was announced earlier in 2024. The technology behind Sora is the Diffusion Transformer (DiT) developed by William Peebles and Shining Xie in 2023.

How does DiT work?

[![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_lossy/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffa4f6ccf-a7f2-4cd0-9906-d4044a8512b7_810x1080.gif)

](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffa4f6ccf-a7f2-4cd0-9906-d4044a8512b7_810x1080.gif)

Walkthrough

-------------

**Goal:**Â Generate a video conditioned by a text prompt and a series of diffusion steps

\[1\] Given
â†³ Video
â†³ Prompt: "sora is sky"
â†³ Diffusion step: t = 3
\[2\] Video â†’ Patches
â†³ Divide all pixels in all frames into 4 spacetime patches
\[3\] Visual Encoder: Pixels ğŸŸ¨Â Â â†’ Latent ğŸŸ©
â†³ Multiply the patches with weights and biases, followed by ReLU
â†³ The result is a latent feature vector per patch
â†³ The purpose is dimension reduction from 4 (2x2x1) to 2 (2x1).
â†³ In the paper, the reduction is 196,608 (256x256x3)â†’ 4096 (32x32x4)
\[4\] â¬› Add Noise
â†³ Sample a noise according to the diffusion time step t. Typically, the larger the t, the smaller the noise.
â†³ Add the Sampled Noise to latent features to obtain Noised Latent.
â†³ The goal is to purposely add noise to a video and ask the model to guess what that noise is.
â†³ This is analogous to training a language model by purposely deleting a word in a sentence and ask the model to guess what the deleted word was.
\[5-7\] ğŸŸª Conditioning by Adaptive Layer Norm
\[5\] Encode Conditions
â†³ Encode "sora is sky" into a text embedding vector \[0,1,-1\].
â†³ Encode t = 3 to as a binary vector \[1,1\].
â†³ Concatenate the two vectors in to a 5D column vector.
\[6\] Estimate Scale/Shift
â†³ Multiply the combined vector with weights and biases
â†³ The goal is to estimate the scale \[2,-1\] and shift \[-1,5\].
â†³ Copy the result to (X) and (+)
\[7\] Apply Scale/Sift
â†³ Scale the noised latent by \[2,-1\]
â†³ Shifted the scaled noised latent by \[-1, 5\]
â†³ The result is "conditioned" noise latent.
\[8-10\] Transformer
\[8\] Self-Attention
â†³ Feed the conditioned noised latent to Query-Key function to obtain a self-attention matrix
â†³ Value is omitted for simplicity
\[9\] Attention Pooling
â†³ Multiply the conditioned noised latent with the self-attention matrix
â†³ The result are attention weighted features
\[10\] Pointwise Feed Forward Network
â†³ Multiply the attention weighted features with weights and biases
â†³ The result is the Predicted Noise
ğŸ‹ï¸â€â™‚ï¸Â **Train**
\[11\]
â†³ Calculate MSE loss gradients by taking the different between the Predicted Noise and the Sampled Noise (ground truth).
â†³ Use the loss gradients to kick off backpropagation to update all learnable parameters (red borders)
â†³ Note the visual encoder and decoder's parameters are frozen (blue borders)
ğŸ¨Â **Generate (Sample)**
\[12\] Denoise
â†³ Subtract the predicted noise from the noised latent to obtain the noise-free latent
\[13\] Visual Decoder: Latent ğŸŸ© â†’ Pixels ğŸŸ¨
â†³ Multiply the patches with weights and biases, followed by ReLU
\[14\] Patches â†’ Video
â†³ Rearrange patches into a sequence of video frames.